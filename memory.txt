Then 48 bits of virtual address are transformed into 52 bits of physical address with the help of special tables
---------------------
cat /proc/bud.. can give memory fragmentation stats
numastat gives memory stats per Numa node
-----------------------
Since meltdown patch, now user-space and kernel-space live are not shared and they use diffrent memory page ranges! AMD don't suffer from it
-------------
Modern x86 microarchitectures have dedicated page-walk hardware. They can even speculatively do page-walks to load TLB entries before a TLB miss actually happen. Skylake can even have two page walks in flight at once.
The page-table loads generated by the page-walk hardware can hit in L1, L2, or L3 caches

--------------
You cant handle a miss nor in user-space neither in kernel-space (on x86 and many other popular platforms). This is because most platforms manages TLB misses in hardware:. MMU (part of CPU/chipset) will do a walk on page tables and will get physical address transparently. Only if some bits are set or when the address region is not mapped, page fault interrupt is generated and delivered to kernel.
--------------------
TLB caches are shared between all PROCESSES! That means context switch may not invalidate it!!
For instance, modern Intel TLBs have a process context ID (PCID); each TLB entry is
 tagged with the PCID of the process that the page mapping is for, and the 
processor will only use that TLB entry if its PCID matches the ID of the 
currently running process. The processor fills the PCID when it creates the
 TLB entry from a register which the operating system must set to a unique 
value for each process

------------
the hardware defines the page table structure, at least on x86 (including x86-64). 
Every process has its own page table structure starting from the Page Global Directory,
 which is pointed to by the CR3 register. When a context switch occurs,
 that is, a new process is selected for execution, a new value for the
 CR3 register is loaded from the new process' descriptor. In addition to
 the process specific user level memory mappings, all processes share a
 common mapping of the kernel address space. This area is accessible only when 
the process is in kernel mode
---------------------
The Wikipedia article on page faults mentions the case were memory is shared by 
different processes as an example which could lead to a soft page fault. 
A soft page fault is a real page fault, and result in calling a 
handler routine to fix the situation. The difference compared to a hard 
page fault is that it is easier to handle, because the data is already in RAM, 
and does not need to be loaded from disk. All that needs to be done is to fix the page 
tables of the faulting process to point to the correct page frame
-----------------------------
This sounds like it's just the cost to traverse the page table,
 right? ~300 cycles per raw memory lookup,
----------------------
A potential source of memory latency is called a minor page fault.
 They are created when a process attempts to access a portion of memory before 
it has been initialized. In this case, the system will need to perform some operations 
to fill the memory maps or other management structures. The severity of a minor page 
fault can depend on system load and other factors, but they are usually short and have 
a negligable impact.
These MAINOR PAGE FAULTS ARE HANDLED LIKE FAULTS!
-----------
huge pages are blocks of memory that come in 2MB and 1GB sizes. 
The page tables used by the 2MB pages are suitable for managing multiple gigabytes of memory-
--------------
The virtual memory statistics can be monitored by looking for the pgfault value in 
the /proc/vmstat file
.
----------
physical page owners -> 
User-space processes,
Dynamically allocated kernel data,
Static kernel code,
Page cache.
----------

 By  default,  Linux  follows  an optimistic memory allocation strategy.
       This means that when malloc() returns non-NULL there  is  no  guarantee
       that  the  memory  really  is available.  This is a really bad bug.  In
       case it turns out that the system is out of memory, one  or  more  pro-
       cesses  will  be  killed  by the infamous OOM killer.  In case Linux is
       employed under circumstances where it would be less desirable  to  sud-
       denly lose some randomly picked processes, and moreover the kernel ver-
       sion is sufficiently recent, one can  switch  off  this  overcommitting
       behavior using a command like:


           # echo 2 > /proc/sys/vm/overcommit_memory

------------
mbind
mlock
/proc/[pid]/pagemap

------
ZONE_DMA
ZONE_NORMAL
-------------
The size of the per-process kernel stacks depends on both the architecture and a compile-time option. Historically, the kernel stack has been two pages per process
ZONE_HIGH(no more on x64)
--------------
malloc provides access to a process's heap. The heap is a construct in the C core library (commonly libc) that allows objects to obtain exclusive access to some space on the process's heap.

Each allocation on the heap is called a heap cell. This typically consists of a header that hold information on the size of the cell as well as a pointer to the next heap cell. This makes a heap effectively a linked list.

When one starts a process, the heap contains a single cell that contains all the heap space assigned on startup. This cell exists on the heap's free list.

When one calls malloc, memory is taken from the large heap cell, which is returned by malloc. The rest is formed into a new heap cell that consists of all the rest of the memory.

When one frees memory, the heap cell is added to the end of the heap's free list. Subsequent mallocs walk the free list looking for a cell of suitable size.

As can be expected the heap can get fragmented and the heap manager may from time to time, try to merge adjacent heap cells.

When there is no memory left on the free list for a desired allocation, malloc calls brk or sbrk which are the system calls requesting more memory pages from the operating system.

Now there are a few modification to optimize heap operations.

For large memory allocations (typically > 512 bytes, the heap manager may go straight to the OS and allocate a full memory page.
The heap may specify a minimum size of allocation to prevent large amounts of fragmentation.
The heap may also divide itself into bins one for small allocations and one for larger allocations to make larger allocations quicker.
There are also clever mechanisms for optimizing multi-threaded heap allocation.

-------------
the kernel developers implemented a new feature: interrupt stacks.
 Interrupt stacks provide a single per-processor
 stack used for interrupt handlers.
 With this option, interrupt handlers no longer share the kernel stack of the interrupted process.
 Instead, they use their own stacks. This consumes only a single page per processor.

------------
Processes may share their address spaces with their children via the CLONE_VM flag to clone().
 The process is then called a thread. This is essentially the only difference between normal processes
 and so-called threads in Linux (Chapter 3); the Linux kernel does not otherwise differentiate 
between them. Threads are regular processes to the kernel that merely share certain resources.

---------------------
o answer another part of the question - the kernel is mapped into every processes address space
 partially for efficiency/performance reasons (there are others too, I'm sure). On most modern hardware,
 it is quicker to change the security level (thus allowing access to the pages that are otherwise protected,
 as mentioned in Alexey's answer) in order to perform system calls and other kernel provided functions than
 it is to change the security level and the entire virtual memory map, along with all the associated TLB cache flushes and everything else involved in a full context switch. Since system calls can be fairly frequent events, the design that has evolved in Linux and many other places is to try to minimize the overhead of utilizing kernel services, and mapping the kernel code and 
(at least some of the) data into each process is part of that.

-----------
When a process runs in kernel mode, it often has to access user
mode memory whose address has been passed by an untrusted program.

To overcome this situation, Linus decided to let the virtual memory
hardware present in every Linux-capable CPU handle this test.

How does this work?

Whenever the kernel tries to access an address that is currently not
accessible, the CPU generates a page fault exception and calls the
page fault handler

void do_page_fault(struct pt_regs *regs, unsigned long error_code)

in arch/x86/mm/fault.c. The parameters on the stack are set up by
the low level assembly glue in arch/x86/kernel/entry_32.S. The parameter
regs is a pointer to the saved registers on the stack, error_code
contains a reason code for the exception.

do_page_fault first obtains the unaccessible address from the CPU
control register CR2. If the address is within the virtual address
space of the process, the fault probably occurred, because the page
was not swapped in, write protected or something similar. However,
we are interested in the other case: the address is not valid, there
is no vma that contains this address. In this case, the kernel jumps
to the bad_area label.

---------
in modern malloc, when 2 threads call malloc at the same time, memory is allocated immediately, since each thread maintains a separate heap, and their own freelist chunk data structure



